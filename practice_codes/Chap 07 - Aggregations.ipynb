{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd6735c8-7d25-414d-b18f-d48b6b5db21e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **Aggregation** function must produce one result for each group, given multiple input values.\n",
    "* Spark also allows us to create the following groupings types\n",
    "  * The **simplest grouping** is to just summarize a complete DataFrame by performing an aggregation in a select statement.\n",
    "  * A **group by** allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns.\n",
    "  * A **window** gives you the ability to specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row.\n",
    "  * A **grouping set,** which you can use to aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n",
    "  * A **rollup** makes it possible for you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.\n",
    "  * A **cube** allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns.\n",
    "\n",
    "* Each grouping returns a RelationalGroupedDataset on which we specify our aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bff97489-174a-4883-a8d8-0d93c0c86993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 11:35:37 WARN Utils: Your hostname, tars resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface wlan0)\n",
      "22/10/31 11:35:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 11:35:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Aggregations').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dadf4889-4ce4-4768-88a6-de1dc73ba12c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading in our data on purchases, repartitioning the data to have far fewer partitions (because we know it’s a small volume of data stored in a lot of small files), and caching the results for rapid access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03473324-70c7-4fe7-acda-f278f68cc2d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"data/2010-12-01.csv\")\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cc5a14a-785d-4ee0-8f5f-18601a090483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "508e721f-b214-48b8-a02f-b8fe82c5b75d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b0bb25c-d784-468c-8251-e8b98247cb19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Count\n",
    "* specify a specific column to count, or \n",
    "* all the columns by using count(*) or count(1) to represent that we want to count every row as the literal one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6949348f-41bf-48c2-bea0-bd32abd427a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|            3108|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e65a2bcb-cb67-41c7-9e97-ff45c1f77675",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note:**\n",
    "\n",
    "when performing a count(*), Spark will count null values (including rows containing all nulls). \n",
    "However, when counting an individual column, Spark will not count the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b413202e-4672-438e-825e-1c1c6bc5f6d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**countDistinct:** To get the number of unique groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd7fb6a5-5ce8-45f4-8bc3-10da902ddc9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     1351|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05ca8483-37b7-46dc-a1f0-a090ae6f013a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**approx_count_distinct:**\n",
    "\n",
    "* use this function when an approximation to a certain degree of accuracy will work just fine\n",
    "* Another parameter with which you can specify the maximum estimation error allowed.\n",
    "* We can see much performance with larger datasets. specifying the large error makes an answer that is quite far off but does complete more quickly than countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "070ca119-6821-4cb4-83de-5c33e4b6487a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 11:36:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            1359|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "852e0d93-8738-4ae4-988b-92fa6f2b9be0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            1282|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69fa56eb-d990-4c40-aa21-7f1c87a84246",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**first and last:**\n",
    "\n",
    "* Used to get the first and last values from a DataFrame.\n",
    "* This will be based on the rows in the DataFrame, not on the values in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26ae8adf-7961-4638-a09d-fec8ef4846c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          20755|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86933641-8006-41a1-a896-f595d799a125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**min and max:**\n",
    "\n",
    "use min and max to extract the minimum and maximum values from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15d85681-ff0c-42dc-a7e7-d870dca79614",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|          -24|          600|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "\n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcb4d52f-6027-4bed-a4d3-b2faf3a1932c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(StockCode)|max(StockCode)|\n",
      "+--------------+--------------+\n",
      "|         10002|          POST|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"StockCode\"),max(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a7bfc15-b344-4d27-acf3-afa2823d3605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**SUM:**\n",
    "\n",
    "add all the values in a row using the sum function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ba7268b-18b9-4f52-a66d-eff81149bc70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|        26814|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00db81e2-b995-4eb8-8695-b46a404e8aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**sumDistinct:**\n",
    "By using this function, we can sum a distinct set of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb046b70-6a4a-4414-bfab-d4e6b1993cdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                  4690|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum_distinct\n",
    "\n",
    "df.select(sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d4bc27d-ffab-47be-b9b2-852bedba4abb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**avg:**\n",
    "Although you can calculate average by dividing sum by count, Spark provides an easier way to get that value via the avg or mean functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6eff2682-b9ce-4c1c-951f-c57123e5aa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+-----------------+\n",
      "|(total_purchases / total_transactions)|    avg_purchases|   mean_purchases|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "|                     8.627413127413128|8.627413127413128|8.627413127413128|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "df.select(\n",
    "         count(\"Quantity\").alias(\"total_transactions\"),\n",
    "         sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "         avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "         expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "       .selectExpr(\n",
    "                  \"total_purchases/total_transactions\",\n",
    "                  \"avg_purchases\",\n",
    "                  \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e32f88c6-9b68-4c9c-ab57-dfcee8d8f112",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Variance and Standard Deviation:**\n",
    "\n",
    "Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65f288fe-67ab-4ab2-aadf-4b6440a15e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|695.2492099104054| 695.4729785650273|  26.367578764657278|   26.371821677029203|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9d16522-d6f2-4b27-b4c1-c1f74f8d3c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**skewness and kurtosis:**\n",
    "\n",
    "* Skewness measures the asymmetry of the values in your data around the mean.\n",
    "* kurtosis is a measure of the tail of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02b1fe99-5419-48d0-9044-1b160205b00c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|skewness(Quantity)|kurtosis(Quantity)|\n",
      "+------------------+------------------+\n",
      "|11.384721296581182|182.91886804842397|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5ea6d07-2cc2-4255-9606-9d5a41af531c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Covariance and Correlation:**\n",
    "\n",
    "* Correlation measures the Pearson correlation coefficient, which is scaled between –1 and +1.\n",
    "* The covariance is scaled according to the inputs in the data\n",
    "* Covariance can be calculated either as the sample covariance or the population covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7afa977f-f0d1-409e-ac9e-53d422378e29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     -0.12225395743668731|            -235.56327681311157|            -235.4868448608685|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76c11573-b54d-47c2-9ccc-9c7f1b01256a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Aggregating to Complex Types:**\n",
    "In Spark, you can perform aggregations not just of numerical values using formulas, you can also perform them on complex types. \n",
    "\n",
    "For example, we can collect a list of values present in a given column or only the unique values by collecting to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e81ed987-2202-42a3-9c9c-6962d2466f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[France, Australi...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30e1bac6-232d-4268-be87-f49bc0c957ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Grouping\n",
    "This will be done as group our data on one column and perform some calculations on the other columns that end up in that group.\n",
    "We will group by each unique invoice number and get the count of items on that invoice. This will be done in 2 phases\n",
    "\n",
    "* First we specify the column(s) on which we would like to group. This returns **Relatational Grouped Dataset.**\n",
    "* then we specify the aggregation(s).This step returns **DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6e8d26c-4c21-47e7-af79-e95359d26ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fd2f09d7130>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1826ce6-262a-435b-806a-bafc40253715",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|CustomerId|InvoiceNo|count|\n",
      "+----------+---------+-----+\n",
      "|   13468.0|   536564|    2|\n",
      "|      null|   536596|    6|\n",
      "|   17025.0|   536573|    4|\n",
      "|      null|   536414|    1|\n",
      "|   16098.0|   536382|   12|\n",
      "|   14849.0|   536460|   14|\n",
      "|      null|   536550|    1|\n",
      "|   17841.0|  C536543|    2|\n",
      "|   13777.0|   536583|    1|\n",
      "|   12472.0|  C536548|   14|\n",
      "+----------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"CustomerId\", \"InvoiceNo\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fb9d747-dcee-47d4-a269-ea145081a93b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Grouping with Expressions:**\n",
    "Rather than passing count() function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregations specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8db3aa2-c2b2-4f77-8350-179193ea13a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Grouping with Maps:**\n",
    "Sometimes it can be easier to specify your transformations as a series of Maps for which the key is the column and the value is the aggregation function. \n",
    "\n",
    "You can reuse multiple column names if you specify them inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d871d7e-b9c0-405a-a459-8ebdf2bdd0ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+---------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|count(Quantity)|\n",
      "+---------+------------------+--------------------+---------------+\n",
      "|   536596|               1.5|  1.1180339887498947|              6|\n",
      "|   536597|2.5357142857142856|  2.7448932175059566|             28|\n",
      "|   536414|              56.0|                 0.0|              1|\n",
      "+---------+------------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\\\n",
    "                           expr(\"avg(Quantity)\"),\n",
    "                           expr(\"stddev_pop(Quantity)\"),\n",
    "                           expr(\"count(Quantity)\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f4fb460-a65e-455b-b963-e18f269bc427",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Window Functions:\n",
    "\n",
    "window functions are used to carry out some unique aggregations by either computing some aggregation on a specific **window** of data, which you define by using a reference to the current data.\n",
    "\n",
    "This window specification determines which rows will be passed in to this function.\n",
    "\n",
    "A groupBy takes data, and every row can go only into one grouping. A window function calculates a return value for every input row of a table based on group of rows, called a frame. Each row fall into one or more frames.\n",
    "\n",
    "Spark supports three kinds of window functions:\n",
    "* ranking functions - Rank, Dense_Rank, Row_Number etc..,\n",
    "* analytic functions - Lead, Lag,First_value,Last_value etc..,\n",
    "* aggregate functions- sum,avg,count,min,max etc..,\n",
    "\n",
    "OVER clause defines the partitioning and ordering of rows(i.e a window) for the above functions to operate on. Hence these functions are called window functions. The OVER clause accepts the following three arguments to define a window for these functions to operate on.\n",
    "\n",
    "* ORDERBY - Defines the logical order of the rows\n",
    "* PARTITION BY - Divides the query result set in to partitions. The window function is applied to each partition separately.\n",
    "* ROWS or RANGE Clause - Further limits the rows within the partition by specifying start and end points within the partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1dea40d7-4f52-4381-ba7e-0f4ee29f89cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,col\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\",to_date(col(\"InvoiceDate\"),\"MM/d/YYYY H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c5b08750-d4bf-4ec5-9938-08011382a9f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"CustomerId\",\"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)                    \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1da4421a-8a65-4200-8a68-e7f11164be6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need aggregate function to learn more about each specific customer. ex: maximum purchase quantity over all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "013f3203-95a0-4546-8ef8-e7bd067bb10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a24d78c0-c212-49fa-a0b5-b531c2e82142",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|CustomerID|      date|max_purchase|\n",
      "+----------+----------+------------+\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12431.0|2010-12-01|          24|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "|   12433.0|2010-12-01|          96|\n",
      "+----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = F.max(F.col(\"Quantity\")).over(windowSpec)\n",
    "dfWithDate.withColumn('max_purchase', F.max(F.col(\"Quantity\")).over(windowSpec)).select('CustomerID', 'date', 'max_purchase').na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "842a2e72-0ffc-40d2-8e37-33cf8707cc5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "create the purchase quantity rank. For that, we use the **dense_rank** function to determine which date had the maximum purchase quantity\n",
    "for every customer.\n",
    "\n",
    "We use dense_rank as opposed to rank to avoid gaps in the ranking sequence when there are tied values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49ef2112-34c9-4f1d-8930-89624dca9064",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ca217d6-f117-42fd-b7e6-6282ff4ce40f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|   12431.0|2010-12-01|      24|           1|                1|                 24|\n",
      "|   12431.0|2010-12-01|      24|           1|                1|                 24|\n",
      "|   12431.0|2010-12-01|      12|           3|                2|                 24|\n",
      "|   12431.0|2010-12-01|       8|           4|                3|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       3|          11|                6|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12433.0|2010-12-01|      96|           1|                1|                 96|\n",
      "|   12433.0|2010-12-01|      72|           2|                2|                 96|\n",
      "|   12433.0|2010-12-01|      72|           2|                2|                 96|\n",
      "|   12433.0|2010-12-01|      50|           4|                3|                 96|\n",
      "|   12433.0|2010-12-01|      48|           5|                4|                 96|\n",
      "|   12433.0|2010-12-01|      48|           5|                4|                 96|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\\\n",
    "       col(\"CustomerId\"),\n",
    "       col(\"date\"),\n",
    "       col(\"Quantity\"),\n",
    "       purchaseRank.alias(\"quantityRank\"),\n",
    "       purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "       maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26d95aa2-8f0a-4578-acee-3a62b98b11f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CustomerId, date, Quantity,\n",
    "rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "ORDER BY Quantity DESC NULLS LAST\n",
    "ROWS BETWEEN\n",
    "UNBOUNDED PRECEDING AND\n",
    "CURRENT ROW) as rank,\n",
    "dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "ORDER BY Quantity DESC NULLS LAST\n",
    "ROWS BETWEEN\n",
    "UNBOUNDED PRECEDING AND\n",
    "CURRENT ROW) as dRank,\n",
    "max(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "ORDER BY Quantity DESC NULLS LAST\n",
    "ROWS BETWEEN\n",
    "UNBOUNDED PRECEDING AND\n",
    "CURRENT ROW) as maxPurchase\n",
    "FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e82032b5-e08a-46fa-b49d-7388507283fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Grouping Sets\n",
    "* an aggregation across multiple groups.\n",
    "* These are a low-level tool for combining sets of aggregations together.\n",
    "* \n",
    "\n",
    "total quantity of all stock codes and customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b19b3a5c-f676-4113-9fec-8b839f6243e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 14:08:48 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 441493 ms exceeds timeout 120000 ms\n",
      "22/10/31 14:08:48 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa310e62-d3f2-4f99-8fa4-679c745b0ead",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a8074f7-b0b6-49b3-b76f-f94e2b16c611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY GROUPING SETS((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db94e55b-1f5f-4a8e-bb5c-310f144cd326",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "But if we want to include the total number of items, regardless of customer or stockcode ? with conventiopnal group-by statement this impossible. But, its simple with grouping sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21d38517-aa20-44f9-9415-42cb5d2285b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY GROUPING SETS((customerId, stockCode),())\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9de527dc-a3e4-4d2e-8174-13f2d9baab0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The **GROUPING SETS** operator is only available in SQL. To perform the same in DataFrames, we use the rollup and cube operators - which will allow us to get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ffef43a-6099-4b5d-a784-ab18606f75a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Grouping sets depend on **null values** for aggregation levels. If you do not filter-out null values, you will get incorrect results. \n",
    "This applies to cubes, rollups, and grouping sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af07afc3-031a-4a8d-83ca-4103619860f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS(())\n",
    "ORDER BY CustomerId DESC, stockCode DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2271efc4-6497-4be4-a184-59755c57463c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The GROUPING SETS operator is only available in SQL. To perform the same in DataFrames, we need to  use the **rollup** and **cube** operators—which allow us to get the same results.\n",
    "\n",
    "**ROLLUP:** \n",
    "* is used to do aggregate Opereation on multiple levels in a heirarchy.  \n",
    "* is a multidimensional aggregation that performs a variety of group-by style calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f0e7645c-f583-4bfb-958f-cc8fca2415d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\",\"Country\")\\\n",
    "                     .agg(sum(\"Quantity\"))\\\n",
    "                     .selectExpr(\"Date\",\"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "                     .orderBy(\"Date\")\n",
    "rolledUpDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92058b9e-1bf9-4ad1-820a-11e3f404a5d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|         26814|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17df7ce0-3116-460e-8d53-e902fb042bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|         26814|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\" and \"Date IS NULL\").show()\n",
    "#rolledUpDF.where().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f24c9827-7816-4490-bbb7-5c3dd3875f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CUBE:**\n",
    "* Produces the result set by generating all combinations of columns specified in GroupBy CUBE()\n",
    "* Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won’t just go by date over the entire time period, but also the country.\n",
    "* To createa a table that has below combinations\n",
    "  * The total across all dates and countries\n",
    "  * The total for each date across all countries\n",
    "  * The total for each country on each date\n",
    "  * The total for each country across all dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fd3618d-5f8a-4712-a059-ec4726225029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|      Date|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|      null|        France|          449|\n",
      "|      null|          null|        26814|\n",
      "|      null|     Australia|          107|\n",
      "|      null|       Germany|          117|\n",
      "|      null|        Norway|         1852|\n",
      "|      null|United Kingdom|        23949|\n",
      "|      null|          EIRE|          243|\n",
      "|      null|   Netherlands|           97|\n",
      "|2010-12-01|     Australia|          107|\n",
      "|2010-12-01|United Kingdom|        23949|\n",
      "|2010-12-01|          null|        26814|\n",
      "|2010-12-01|   Netherlands|           97|\n",
      "|2010-12-01|        France|          449|\n",
      "|2010-12-01|        Norway|         1852|\n",
      "|2010-12-01|       Germany|          117|\n",
      "|2010-12-01|          EIRE|          243|\n",
      "+----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cubeDF = dfNoNull.cube(\"Date\",\"Country\")\\\n",
    "        .agg(sum(\"Quantity\"))\\\n",
    "        .select(\"Date\",\"Country\",\"sum(Quantity)\")\\\n",
    "        .orderBy(\"Date\")\n",
    "cubeDF.count()\n",
    "cubeDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d00c2467-335f-414e-b98d-e6e9f87ed00d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Difference Between CUBE and ROLLUP**\n",
    "* CUBE generates a result that shows aggregation for all combinations of values in the selected columns.\n",
    "* ROLLUP generates a result that shows aggregation for hierarchy of values in the selected columns.\n",
    "\n",
    "ROLLUP(\"Date\",\"Country\") gives combinations of\n",
    "  * \"Date\",\"Country\"\n",
    "  * \"Date\"\n",
    "  * ()\n",
    "It gave a count of 2022\n",
    "\n",
    "cube(\"Date\",\"Country\") gives combination of\n",
    "  * \"Date\",\"Country\"\n",
    "  * \"Date\"\n",
    "  * \"Country\"\n",
    "  * ()\n",
    "It gave a count of 2060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1d09aba-db42-4984-b75b-811e543bbc43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**GROUPING METADATA:**\n",
    "Sometimes when using cubes and rollups, you want to be able to query the aggregation levels so that you can easily filter them down accordingly.\n",
    "We can do this by using the grouping_id, which gives us a column specifying the level of aggregation that we have in our result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "daddb4f0-9a3a-4193-bfc0-cb8947f3319c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|      null|     null|            3|        26814|\n",
      "|      null|    21594|            2|            1|\n",
      "|      null|    22695|            2|           17|\n",
      "|      null|    22090|            2|            2|\n",
      "|      null|    21724|            2|           12|\n",
      "|      null|    22943|            2|           11|\n",
      "|      null|    21257|            2|            4|\n",
      "|      null|    22382|            2|           15|\n",
      "|      null|    22757|            2|            3|\n",
      "|      null|    22087|            2|            3|\n",
      "|      null|   47559B|            2|            2|\n",
      "|      null|    22666|            2|           28|\n",
      "|      null|    21147|            2|            4|\n",
      "|      null|   85039A|            2|            3|\n",
      "|      null|    85172|            2|           48|\n",
      "|      null|    37509|            2|            1|\n",
      "|      null|    21670|            2|            6|\n",
      "|      null|    21756|            2|           14|\n",
      "|      null|    21913|            2|           13|\n",
      "|      null|   84031A|            2|           10|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import grouping_id,desc\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\")\\\n",
    "        .agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "        .orderBy(desc(\"grouping_id()\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e27e9ec-47f2-42fc-8975-d2da17701a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**PIVOT:**\n",
    "* Used to turn unique values from one column, into multiple columns in the output.\n",
    "* With a pivot, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9e84612-7e19-4b95-af13-49c6d12ca4da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+------------------------+-------------------------+------------------+-------------------+--------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+\n",
      "|      date|Australia_sum(Quantity)|Australia_sum(UnitPrice)|Australia_sum(CustomerID)|EIRE_sum(Quantity)|EIRE_sum(UnitPrice)|EIRE_sum(CustomerID)|France_sum(Quantity)|France_sum(UnitPrice)|France_sum(CustomerID)|Germany_sum(Quantity)|Germany_sum(UnitPrice)|Germany_sum(CustomerID)|Netherlands_sum(Quantity)|Netherlands_sum(UnitPrice)|Netherlands_sum(CustomerID)|Norway_sum(Quantity)|Norway_sum(UnitPrice)|Norway_sum(CustomerID)|United Kingdom_sum(Quantity)|United Kingdom_sum(UnitPrice)|United Kingdom_sum(CustomerID)|\n",
      "+----------+-----------------------+------------------------+-------------------------+------------------+-------------------+--------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+\n",
      "|2010-12-01|                    107|                    73.9|                 174034.0|               243| 133.64000000000001|            313131.0|                 449|                55.29|              251660.0|                  117|     93.82000000000002|               364538.0|                       97|                     16.85|                    25582.0|                1852|               102.67|              907609.0|                       23949|           12428.080000000024|                   2.8785059E7|\n",
      "+----------+-----------------------+------------------------+-------------------------+------------------+-------------------+--------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()\n",
    "pivoted.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8787b6b0-0d84-4c5f-bb86-15b5aeb78963",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. \n",
    "\n",
    "For example, for USA we have columns:USA_sum(Quantity), USA_sum(UnitPrice), USA_sum(CustomerID).\n",
    "\n",
    "This represents one for each numeric column in our dataset (because we just performed an aggregation over all of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a0e47f7-9dff-4aee-818b-43d3ea75e58a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------------------+\n",
      "|      date|Germany_sum(CustomerID)|Germany_sum(Quantity)|\n",
      "+----------+-----------------------+---------------------+\n",
      "|2010-12-01|               364538.0|                  117|\n",
      "+----------+-----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date >= '2010-12-01'\").select(\"date\",\"`Germany_sum(CustomerID)`\",\"`Germany_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36f8434e-90ab-4cca-825e-1f40ca4e91bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### User-Defined Aggregation Functions(UDAF):\n",
    "\n",
    "* UDAFs to compute custom calculations over groups of input data (as opposed to single rows).\n",
    "* they are a way for users to define their own aggregation functions based on custom formulae or business rules.\n",
    "* UDAFs are currently available only in Scala or Java. But these can be called in Python.\n",
    "* To create a UDAF, you must inherit from the **UserDefinedAggregateFunction** base class and implement the following methods:\n",
    "   * **inputSchema** represents input arguments as a StructType\n",
    "   * **bufferSchema** represents intermediate UDAF results as a StructType\n",
    "   * **dataType** represents the return DataType\n",
    "   * **deterministic** is a Boolean value that specifies whether this UDAF will return the same result for a given input\n",
    "   * **initialize** allows you to initialize values of an aggregation buffer\n",
    "   * **update** describes how you should update the internal buffer based on a given row\n",
    "   * **merge** describes how two aggregation buffers should be merged\n",
    "   * **evaluate** will generate the final result of the aggregation\n",
    "   \n",
    " The following example implements a BoolAnd, which will inform us whether all the rows (for a given column) are true; if they’re not, it will return false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4167763d-db6a-429f-9477-7cf33b65f1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "  def inputSchema: org.apache.spark.sql.types.StructType = StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "  \n",
    "  def bufferSchema: StructType = StructType(StructField(\"result\", BooleanType) :: Nil)\n",
    "  \n",
    "  def dataType: DataType = BooleanType\n",
    "  \n",
    "  def deterministic: Boolean = true\n",
    "  \n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = true\n",
    "  }\n",
    "  \n",
    "  def update(buffer: MutableAggregationBuffer, input: Row):Unit = {\n",
    "    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "  }\n",
    "  \n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "    }\n",
    "  \n",
    "  def evaluate(buffer: Row): Any = {\n",
    "    buffer(0)\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8506c9b3-e5ec-4cd9-a045-c8d4ffd46e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    ".selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    ".selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    ".select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7d61a4e-0ee8-404c-a6f4-ee8c9d3c10e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Conclusion:\n",
    "We have learnt about simple grouping-to window functions as well as rollups and cubes. Next chapterdiscusses how to perform joins to combine different data sources together."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Chap 07 - Aggregations",
   "notebookOrigID": 3965567339510461,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
