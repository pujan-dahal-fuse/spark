{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcbd9543-573a-423f-9e38-7be337e1be08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Join Expressions:\n",
    "* Join brings two datasets together by comparing by comparing the value of one or more keys of the left and right and evaluating the result of a join expression\n",
    "* Join expression determines whether Spark should bring together the left set of data with the right set of data.\n",
    "* an **equi-join expression**, compares whether the specified keys in your left and right datasets are equal.\n",
    "* Spark also allows for much more sophsticated join policies in addition to equi-joins. We can even use **complex types and perform something like checking whether a key exists within an array when you perform a join.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdd0b574-71ae-409e-9b60-410c915ef1ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Join Types:\n",
    "join type determines what should be in the result set. There are a variety of different join types available in Spark\n",
    "\n",
    "* Inner Joins - Keep rows with keys that exist in the left and right datasets\n",
    "* Outter Joins - Keep rows with keys in either the left or right datasets\n",
    "* Left Outer Joins - Keep rows with keys in the left dataset\n",
    "* Right Outer Joins - Keep rows with keys in the right dataset\n",
    "* Left Semi Join - Keep the rows in the left, and only the left, dataset where the key appears in the right dataset.\n",
    "* Left anti Join - keep the rows in the left, and only the left, dataset where they do not appear in the right dataset.\n",
    "* Natural joins - Perform a join by implicitly matching the columns between the two datasets with the same names.\n",
    "* Cross (or Cartesian) joins - Match every row in the left dataset with every row in the right dataset.\n",
    "\n",
    "letâ€™s create some simple datasets to understand more with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "279f467a-1e49-47f4-803c-31c72bcd051d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 11:59:07 WARN Utils: Your hostname, tars resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface wlan0)\n",
      "22/10/19 11:59:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 11:59:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('joins').getOrCreate()\n",
    "\n",
    "person = spark.createDataFrame([\n",
    "        (0, \"Bill Chambers\", 0, [100]),\n",
    "        (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "        (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "        .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "          (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "          (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "          (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "          .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "              (500, \"Vice President\"),\n",
    "              (250, \"PMC Member\"),\n",
    "              (100, \"Contributor\")])\\\n",
    "              .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6558cd7-be57-4e6c-a73f-9faac4bc3994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f6b9434-cff8-4cf1-8570-e81ef29e41e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()\n",
    "graduateProgram.show()\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "932ac882-878b-41b7-8cf2-4ed92fd49309",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Inner Joins:\n",
    "Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together) only the rows that evaluate to true.\n",
    "\n",
    "Eg: we join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44ae58da-f708-4479-827d-5f9f1c644f5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b2d46d2-1eec-4c1b-a552-ccdcba626994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c31d267d-c108-42e2-8632-83297b3d9d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25c5bab9-5ec6-4283-8fda-fac335610e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57202707-3f8f-44d6-8708-5414a6a72216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac48eac1-b182-4be6-92e5-e1d87a2e577c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1277972674.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [9], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    SELECT * FROM person INNER JOIN graduateProgram ON person.graduate_program = graduateProgram.id\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM person INNER JOIN graduateProgram ON person.graduate_program = graduateProgram.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ff8a09e-89cf-4ffd-95a2-d300c9668117",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Outer Join:\n",
    "* Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. \n",
    "* If there is no equivalent row in either the left or right DataFrame, Spark will insert null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80f7658c-10ee-4902-ac52-4ce19481182a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79e9ddcf-b14c-4db4-ad6d-d40dce5821ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Left Outer Joins:\n",
    "* Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame.\n",
    "* If there is no equivalent row in the right DataFrame, Spark will insert null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ee97336-16ab-48f3-9bb9-ef1bdbf01ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b99c61ee-daa5-416c-b6c7-13386c6223a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Right Outer Joins:\n",
    "* Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame.\n",
    "* If there is no equivalent row in the left DataFrame, Spark will insert null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30054af0-4b44-4638-b287-6e0c8d7fc903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "057ba338-9f26-4184-a894-561ca04e64b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Left Semi Join:\n",
    "* This join actually include doesnt any values from the right DataFrame.\n",
    "* They only compare to see if value exists in second dataframe.\n",
    "* If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "642bfefb-ee84-40a0-9471-981aceebcdb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce41818a-cf9f-40ff-866d-8a5e33494c1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding duplicate row to check how the duplicate works\n",
    "\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "                      (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5ce44d3-aa18-4bdf-9670-41a23fe2794c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Left Anti Joins:\n",
    "* This join is exactly opposite to Left Semi Join.\n",
    "* This also compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a corresponding key in the second DataFrame.\n",
    "* Think of anti joins as a NOT IN SQL-style filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "090aa09f-566d-4223-b2fd-d70b6d7a2d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23168b00-c5dd-4b22-8e41-1329bd08f33f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Cross (Cartesian) Joins:\n",
    "* Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame.\n",
    "* This will cause an absolute explosion in the number of rows contained in the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82953b68-b2d2-42ce-b639-f7101a896aee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33ec5fc8-74e5-4e70-b95e-ad77bf117e22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 73:===================================>                    (12 + 7) / 19]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b456442e-7dd6-4670-a786-a38043cb75fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* You should use cross-joins only if you are absolutely, 100 percent sure that this is the join you need.Reason being we need to be explicit when defining a cross-join in Spark.\n",
    "\n",
    "* Advanced users can set the session-level configuration **spark.sql.crossJoin.enable** to true in order to allow cross-joins without warnings or without Spark trying to perform another join for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41602c2d-07ad-4743-b75a-b9efaefe8fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Challenges When Using Joins:\n",
    "**Joining Complex Types:**\n",
    "* Any expression is a valid join expression, assuming that it returns a Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09916e67-f808-4534-8df1-8deb3a9920ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:======================================>                 (13 + 6) / 19]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\").join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9753556-5f99-418b-b6c4-1fa57b62e25c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Handling Duplicate Column Names:**\n",
    "\n",
    "* In a DataFrame, each column has a unique ID within Sparkâ€™s SQL Engine,Catalyst. This unique ID is purely internal and not something that you can directly reference. \n",
    "* Duplicate column names occur in two distinct situations:\n",
    "  * The Join expression that you specify does not remove one key from one of the input DataFrames and the keys have the same column name.\n",
    "  * Two columns on which we are not performing the join have the same name.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "46094f81-58d5-455a-a054-aacffb423eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()\n",
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57a43291-f1af-437e-8ee9-b3c1b07d9528",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "joinExpr = gradProgramDupe[\"graduate_program\"] == person[\"graduate_program\"]\n",
    "\n",
    "#joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "483f8594-a0ef-4b3d-8a56-f40438489943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "002849ee-42be-4b7c-8db7-8bc358043f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The challenge arises when we refer to one of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce4d2356-d743-4f5c-8201-818b320401b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradProgramDupe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoinExpr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraduate_program\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program."
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a08e89b2-0af0-4ef1-b0ba-e76b519fdef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Approach 1: Different join expression**\n",
    "The easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of\n",
    "the columns for you during the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab320826-ce8a-4a10-bcd8-8adc08bc7d04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|graduate_program| id|            name|   spark_status| degree|          department|     school|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe,\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae0f1e64-fe8c-42a8-a5be-71e0d5d973c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af99e58e-3089-4ae7-b258-5bea4a6b9e84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Approach 2: Dropping the column after the join**\n",
    "* When doing this, we need to refer to the column via the original source DataFrame.\n",
    "* We can do this if the join uses the same key names or if the source DataFrames have columns that simply have the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6735d8c-e483-4e68-a08c-6779798cb70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person[\"graduate_program\"]).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df97a6da-3afa-4533-b5e0-3788e9eac71b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is an artifact of Sparkâ€™s SQL analysis process in which an explicitly referenced column will pass analysis because Spark has no need to resolve the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17d4e624-e2d7-4d74-9028-d3445a5486d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Approach 3: Renaming a column before the join:**\n",
    "\n",
    "We can avoid this issue altogether if we rename one of our columns before the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f49ef75-47b7-49e5-9360-2d733caf6fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "joinExpr = person[\"graduate_program\"] == gradProgram3[\"grad_id\"]\n",
    "person.join(gradProgram3, joinExpr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b13ef780-8fda-4ee7-8d1e-22762d5bf47e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### How Spark Performs Joins:\n",
    "To understand how Spark performs joins, we need to understand the two core resources at play:\n",
    "* node-to-node communication strategy\n",
    "* per node computation stratergy\n",
    "\n",
    "Spark approaches cluster communication in two different ways during joins. It either incurs a\n",
    "* shuffle join, which results in an all-to-all communication. When you join a big table to another big table, you end up with a shuffle join. \n",
    "* a broadcast join\n",
    "\n",
    "The core foundation of simplified view of joins is that in Spark we will have either a **big table** or a **small table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d6228cfe-c516-4fe8-a841-995e0a07afad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Big table to Big table:**\n",
    "* when you join big table to big table, we end up in **Shuffle Join**\n",
    "* In Shuffle join, every node talks to other node and they share data according to which node has a certain key or set of keys(on which you are joining).\n",
    "* These joins are expensive because the network can be become congested with traffic, especially when the data is not partitioned well.\n",
    "\n",
    "![Big table to Bigtable](files/tables/Bigtable_to_bigtable-44cdf.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b14826f-f6e2-44af-8399-dc614997227b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Big tableâ€“toâ€“small table:**\n",
    "* This uses Broadcast Join.\n",
    "* When the table is small enough to fit into the memory of a single worker node, with some breathing room of course, we can optimize our join.\n",
    "* replicate our small DataFrame onto every worker node in the cluster (be it located on one machine or many).\n",
    "* Even it looks expensive initially, but this prevents us from performing the all-to-all communication during the entire join process.\n",
    "* we perform it only once at the beginning and then let each individual worker node perform the work without having to wait or communicate with any other worker node.\n",
    "* This means that joins will be performed on every single node individually, making CPU the biggest bottleneck.\n",
    "\n",
    "![Broad Cast Join](files/tables/BroadCast_Join-1b827.PNG)\n",
    "\n",
    "For our current set of data, we can see that Spark joining type by looking at the explain plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82053e79-8a08-41e0-bdd4-ef876a140ff6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [graduate_program#10L], [id#24L], Inner\n",
      "   :- Sort [graduate_program#10L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(graduate_program#10L, 200), ENSURE_REQUIREMENTS, [id=#2381]\n",
      "   :     +- Project [_1#0L AS id#8L, _2#1 AS name#9, _3#2L AS graduate_program#10L, _4#3 AS spark_status#11]\n",
      "   :        +- Filter isnotnull(_3#2L)\n",
      "   :           +- Scan ExistingRDD[_1#0L,_2#1,_3#2L,_4#3]\n",
      "   +- Sort [id#24L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#24L, 200), ENSURE_REQUIREMENTS, [id=#2382]\n",
      "         +- Project [_1#16L AS id#24L, _2#17 AS degree#25, _3#18 AS department#26, _4#19 AS school#27]\n",
      "            +- Filter isnotnull(_1#16L)\n",
      "               +- Scan ExistingRDD[_1#16L,_2#17,_3#18,_4#19]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(graduateProgram, joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a28db89-5d44-4459-994c-3bf94dd09294",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With DataFrame API, we can also explicitly give the optimizer a hint that we would like to use a broadcast join by using the correct function around the small DataFrame in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d5ea7d8-5267-4bbb-965b-7094f5d07a38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [graduate_program#10L], [id#24L], Inner, BuildRight, false\n",
      "   :- Project [_1#0L AS id#8L, _2#1 AS name#9, _3#2L AS graduate_program#10L, _4#3 AS spark_status#11]\n",
      "   :  +- Filter isnotnull(_3#2L)\n",
      "   :     +- Scan ExistingRDD[_1#0L,_2#1,_3#2L,_4#3]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#2414]\n",
      "      +- Project [_1#16L AS id#24L, _2#17 AS degree#25, _3#18 AS department#26, _4#19 AS school#27]\n",
      "         +- Filter isnotnull(_1#16L)\n",
      "            +- Scan ExistingRDD[_1#16L,_2#17,_3#18,_4#19]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fec51c34-cc11-4d0f-a070-61ef98bb25a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* The SQL interface also includes the ability to provide hints to perform joins. \n",
    "* These are not enforced, however, so the optimizer might choose to ignore them.\n",
    "* We can set one of these hints by using a special comment syntax.\n",
    "    * MAPJOIN\n",
    "    * BROADCAST &\n",
    "    * BROADCASTJOIN \n",
    "   \n",
    "   all do the same thing and are all supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac90d4ab-00d6-4c3b-bf5a-69c1190806d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chambers</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matei Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph.D.</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Armbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph.D.</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "Bill Chambers",
         0,
         [
          100
         ],
         0,
         "Masters",
         "School of Information",
         "UC Berkeley"
        ],
        [
         1,
         "Matei Zaharia",
         1,
         [
          500,
          250,
          100
         ],
         1,
         "Ph.D.",
         "EECS",
         "UC Berkeley"
        ],
        [
         2,
         "Michael Armbrust",
         1,
         [
          250,
          100
         ],
         1,
         "Ph.D.",
         "EECS",
         "UC Berkeley"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "graduate_program",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "spark_status",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "degree",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "school",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT /*+ MAPJOIN(graduateProgram) */ * FROM person JOIN graduateProgram\n",
    "ON person.graduate_program = graduateProgram.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be928579-99a8-49b8-a803-d5316f4c42e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "if you try to broadcast something too large, you can crash your driver node (because that collect is expensive).\n",
    "\n",
    "**Little tableâ€“toâ€“little table:**\n",
    "When performing joins with small tables, itâ€™s usually best to let Spark decide how to join them. You can always force a broadcast join if youâ€™re noticing strange behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdd79be9-5a8f-4002-8fb5-f6f019014346",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Conclusion:\n",
    "It is important to consider is if you **partition** your data correctly prior to a join, you can end up with much more efficient execution because even if a shuffle is planned, if data from two different DataFrames is already located on the same machine, Spark can avoid the\n",
    "shuffle.\n",
    "\n",
    "There are additional implications when you decide what order joins should occur in. Because some joins act as filters, this can be a low-hanging improvement in your workloads, as you are guaranteed to reduce data exchanged over the network."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Chap 08- Joins",
   "notebookOrigID": 3965567339510263,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
