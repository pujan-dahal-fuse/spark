{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c22a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/02 13:07:55 WARN Utils: Your hostname, tars resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface wlan0)\n",
      "22/11/02 13:07:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/02 13:07:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('weather').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd7cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:========>                                                  (1 + 6) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+---------+----------+----+----+----+----+----+------+\n",
      "|      date|     country|        city| Latitude| Longitude|tavg|tmin|tmax|wdir|wspd|  pres|\n",
      "+----------+------------+------------+---------+----------+----+----+----+----+----+------+\n",
      "|31-12-2021|Vatican City|Vatican City|41.902179| 12.453601| 9.7| 3.7|18.7|  28| 4.9|1025.0|\n",
      "|31-12-2021|   Venezuela|     Caracas|10.480594|-66.903606|26.3|23.8|31.8|  59| 5.9|1013.8|\n",
      "|31-12-2021|     Vietnam|       Hanoi|21.027764| 105.83416|14.7|12.6|16.6|  26| 9.2|1025.0|\n",
      "+----------+------------+------------+---------+----------+----+----+----+----+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "my_df = spark.read.option('header', 'true').option('inferSchema', 'true').csv('./data/daily_weather_data.csv')\n",
    "\n",
    "my_df.orderBy(desc('date')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0354f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- tavg: double (nullable = true)\n",
      " |-- tmin: double (nullable = true)\n",
      " |-- tmax: double (nullable = true)\n",
      " |-- wdir: integer (nullable = true)\n",
      " |-- wspd: double (nullable = true)\n",
      " |-- pres: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b254359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+--------+---------+----+-----+----+----+----+------+\n",
      "|      date|country|city|Latitude|Longitude|tavg| tmin|tmax|wdir|wspd|  pres|\n",
      "+----------+-------+----+--------+---------+----+-----+----+----+----+------+\n",
      "|01-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 2.8|  0.4| 3.6| 174| 8.9|1012.9|\n",
      "|02-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 2.9|  2.3| 3.6| 149|11.4|1012.5|\n",
      "|03-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 4.0|  2.5| 5.6| 137|11.4|1007.9|\n",
      "|04-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 4.1|  2.3| 5.6| 154| 8.4|1005.1|\n",
      "|05-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 3.2|  2.0| 4.6| 173| 8.1|1008.3|\n",
      "|06-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 3.3|  0.9| 4.8| 208| 9.9|1013.9|\n",
      "|07-01-2018|Ukraine|Kiev| 50.4501|  30.5234| 5.4|  3.6| 7.9| 282|10.1|1020.1|\n",
      "|08-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-1.1| -2.4| 2.4| 310|12.2|  null|\n",
      "|09-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-1.6| -3.4| 2.6| 289|15.0|  null|\n",
      "|10-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-2.0| -5.4| 1.7|null| 7.1|  null|\n",
      "|11-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-1.5| -4.0|-0.3|null| 8.3|1022.5|\n",
      "|12-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-1.5| -2.8| 0.6|  51|14.0|1030.7|\n",
      "|13-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-5.0| -6.5|-3.9|  60|14.4|1040.4|\n",
      "|14-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-7.6| -9.4|-6.4|  67|12.2|1038.3|\n",
      "|15-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-9.7|-13.4|-5.9|  70| 8.4|1033.9|\n",
      "|16-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-7.8|-11.4|-5.0| 118| 9.3|1018.7|\n",
      "|17-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-3.5| -7.4|-0.4| 155|11.9|1000.5|\n",
      "|18-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-1.8| -3.7| 0.6| 348|10.5|1001.3|\n",
      "|19-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-0.3| -2.4| 0.6|null|10.5|  null|\n",
      "|20-01-2018|Ukraine|Kiev| 50.4501|  30.5234|-0.9| -4.4| 2.9| 226|14.3|  null|\n",
      "+----------+-------+----+--------+---------+----+-----+----+----+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "my_df.filter(col('country') == 'Ukraine').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9d588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324647\n"
     ]
    }
   ],
   "source": [
    "print(my_df.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
