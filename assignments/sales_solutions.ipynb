{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76303d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total cost, total revenue, total profit on the basis of each region\n",
    "# Find the Item List on the basis of each country\n",
    "# Find the total number of items sold in each country\n",
    "# Find the top five famous item lists on the basis of each region.(Consider units sold while doing this.)\n",
    "# Find all the regions and their famous sales channels.\n",
    "# Find  the list of countries and items and their respective units.\n",
    "# In 2013, identify the regions which sold maximum and minimum units of item type Meat.\n",
    "# List all the items whose unit cost is less than 500\n",
    "# Find the total cost, revenue and profit of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1803b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2df1f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 15:01:56 WARN Utils: Your hostname, tars resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface wlan0)\n",
      "22/10/21 15:01:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 15:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/21 15:01:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f77b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item Type: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Sales Channel: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Total Cost: string (nullable = true)\n",
      " |-- Total Profit: string (nullable = true)\n",
      " |-- Total Revenue: string (nullable = true)\n",
      " |-- Unit Cost: string (nullable = true)\n",
      " |-- Unit Price: string (nullable = true)\n",
      " |-- Units Sold: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------+----------+---------+--------------+--------------------+-------------+----------+----------+------------+-------------+---------+----------+----------+---------------+\n",
      "|             Country|    Item Type|Order Date| Order ID|Order Priority|              Region|Sales Channel| Ship Date|Total Cost|Total Profit|Total Revenue|Unit Cost|Unit Price|Units Sold|_corrupt_record|\n",
      "+--------------------+-------------+----------+---------+--------------+--------------------+-------------+----------+----------+------------+-------------+---------+----------+----------+---------------+\n",
      "|          Azerbaijan|       Snacks| 10/8/2014|535113847|             C|Middle East and N...|       Online|10/23/2014|  91008.96|    51500.76|    142509.72|    97.44|    152.58|       934|           null|\n",
      "|              Panama|    Cosmetics| 2/22/2015|874708545|             L|Central America a...|      Offline| 2/27/2015|1198414.83|   791282.37|   1989697.20|   263.33|    437.20|      4551|           null|\n",
      "|Sao Tome and Prin...|       Fruits| 12/9/2015|854349935|             M|  Sub-Saharan Africa|      Offline| 1/18/2016|  69103.12|    24066.26|     93169.38|     6.92|      9.33|      9986|           null|\n",
      "|Sao Tome and Prin...|Personal Care| 9/17/2014|892836844|             M|  Sub-Saharan Africa|       Online|10/12/2014| 516717.06|   228497.08|    745214.14|    56.67|     81.73|      9118|           null|\n",
      "|              Belize|    Household|  2/4/2010|129280602|             H|Central America a...|      Offline|  3/5/2010|2943879.32|   970846.34|   3914725.66|   502.54|    668.27|      5858|           null|\n",
      "+--------------------+-------------+----------+---------+--------------+--------------------+-------------+----------+----------+------------+-------------+---------+----------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.format('json').load('data/sales_records.json')\n",
    "sales_df.printSchema()\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41bf66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item Type: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Sales Channel: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Total Cost: float (nullable = true)\n",
      " |-- Total Profit: float (nullable = true)\n",
      " |-- Total Revenue: float (nullable = true)\n",
      " |-- Unit Cost: float (nullable = true)\n",
      " |-- Unit Price: float (nullable = true)\n",
      " |-- Units Sold: integer (nullable = true)\n",
      " |-- Corrupt Record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all records are in the form of strings, so we need to do type casting\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "sales_df = sales_df\\\n",
    "            .withColumn('Total Cost', col('Total Cost').cast(FloatType()))\\\n",
    "            .withColumn('Total Profit', col('Total Profit').cast(FloatType()))\\\n",
    "            .withColumn('Total Revenue', col('Total Revenue').cast(FloatType()))\\\n",
    "            .withColumn('Unit Cost', col('Unit Cost').cast(FloatType()))\\\n",
    "            .withColumn('Unit Price', col('Unit Price').cast(FloatType()))\\\n",
    "            .withColumn('Units Sold', col('Units Sold').cast(IntegerType()))\\\n",
    "            .withColumnRenamed('_corrupt_record', 'Corrupt Record')\n",
    "\n",
    "sales_df = sales_df.na.drop(subset=['Region'])\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af349e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to csv\n",
    "sales_df.toPandas().to_csv('output/sales/sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216ff32",
   "metadata": {},
   "source": [
    "## 1. Find the total cost, total revenue, total profit on the basis of each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7652b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:===================>                                       (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              Region|          Total Cost|       Total Revenue|        Total Profit|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Middle East and N...|1.194003115362056...|1.691834583280536...| 4.978314680986322E9|\n",
      "|Australia and Oce...| 7.526098663258995E9|1.070152222371284...|3.1754235603909473E9|\n",
      "|              Europe|2.415937816242763E10|3.423977049206286...|1.008039233312204...|\n",
      "|  Sub-Saharan Africa|2.465031758112777...|3.495487197307492E10|1.030455438786905...|\n",
      "|Central America a...|1.026651963972375...|1.455373016365411...|4.2872105216859627E9|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sum_, col\n",
    "\n",
    "totals_df = sales_df\\\n",
    "    .groupBy('Region')\\\n",
    "    .agg(sum_(col('Total Cost')).alias('Total Cost'), sum_(col('Total Revenue')).alias('Total Revenue'), sum_('Total Profit').alias('Total Profit'))\n",
    "\n",
    "totals_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c8f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "totals_df.toPandas().to_csv('output/sales/totals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c02cb",
   "metadata": {},
   "source": [
    "## 2. Find the Item List on the basis of each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557aadc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===================>                                       (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    Country|           Item List|\n",
      "+-----------+--------------------+\n",
      "|Afghanistan|[Beverages, Perso...|\n",
      "|    Albania|[Beverages, Perso...|\n",
      "|    Algeria|[Beverages, Perso...|\n",
      "|    Andorra|[Beverages, Perso...|\n",
      "|     Angola|[Beverages, Perso...|\n",
      "+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# assuming item type list\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.functions import collect_list, collect_set, explode\n",
    "\n",
    "window_spec = W.partitionBy('Country')\n",
    "country_item_list_df = sales_df\\\n",
    "                        .withColumn('Item List', collect_set('Item Type').over(window_spec))\\\n",
    "                        .select('Country', 'Item List')\\\n",
    "                        .distinct()\n",
    "\n",
    "country_item_list_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04ebc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "country_item_list_df.toPandas().to_csv('output/sales/item_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebfa77",
   "metadata": {},
   "source": [
    "## 3. Find the total number of items sold in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06c59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|Country|Num items sold|\n",
      "+-------+--------------+\n",
      "|   Chad|       2660461|\n",
      "| Russia|       2579558|\n",
      "|  Yemen|       2966519|\n",
      "|Senegal|       2716010|\n",
      "| Sweden|       2698756|\n",
      "+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we need to use units sold for this\n",
    "from pyspark.sql.functions import sum as sum_\n",
    "total_items_df = sales_df\\\n",
    "    .groupBy('Country')\\\n",
    "    .agg(sum_('Units Sold').alias('Num items sold'))\n",
    "\n",
    "total_items_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f9d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "total_items_df.toPandas().to_csv('output/sales/total_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983067c7",
   "metadata": {},
   "source": [
    "## 4. Find the top five famous items list on the basis of each region.(Consider units sold while doing this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ceae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---+\n",
      "|              Region|      Item Type| rn|\n",
      "+--------------------+---------------+---+\n",
      "|                Asia|         Cereal|  1|\n",
      "|                Asia|         Snacks|  2|\n",
      "|                Asia|Office Supplies|  3|\n",
      "|                Asia|     Vegetables|  4|\n",
      "|                Asia|        Clothes|  5|\n",
      "|Australia and Oce...|  Personal Care|  1|\n",
      "|Australia and Oce...|     Vegetables|  2|\n",
      "|Australia and Oce...|         Cereal|  3|\n",
      "|Australia and Oce...|      Beverages|  4|\n",
      "|Australia and Oce...|        Clothes|  5|\n",
      "|Central America a...|      Cosmetics|  1|\n",
      "|Central America a...|        Clothes|  2|\n",
      "|Central America a...|           Meat|  3|\n",
      "|Central America a...|Office Supplies|  4|\n",
      "|Central America a...|      Baby Food|  5|\n",
      "|              Europe|         Cereal|  1|\n",
      "|              Europe|Office Supplies|  2|\n",
      "|              Europe|     Vegetables|  3|\n",
      "|              Europe|      Beverages|  4|\n",
      "|              Europe|         Fruits|  5|\n",
      "+--------------------+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              Region|           Item List|\n",
      "+--------------------+--------------------+\n",
      "|                Asia|[Cereal, Snacks, ...|\n",
      "|Australia and Oce...|[Personal Care, V...|\n",
      "|Central America a...|[Cosmetics, Cloth...|\n",
      "|              Europe|[Cereal, Office S...|\n",
      "|Middle East and N...|[Office Supplies,...|\n",
      "|       North America|[Cosmetics, House...|\n",
      "|  Sub-Saharan Africa|[Cosmetics, Baby ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, rank\n",
    "\n",
    "region_sales_df = sales_df\\\n",
    "                    .groupBy('Region', 'Item Type')\\\n",
    "                    .agg(sum_('Units Sold').alias('Total Units Sold'))\\\n",
    "                    .orderBy('Region')\n",
    "\n",
    "window_spec = W.partitionBy('Region')\\\n",
    "                .orderBy(col('Total Units Sold').desc())\n",
    "\n",
    "region_sales_ranked_df = region_sales_df\\\n",
    "                            .withColumn('rn', rank().over(window_spec))\\\n",
    "                            .where('rn <= 5')\\\n",
    "#                             .drop('rn')\n",
    "\n",
    "# regions and their five best item types\n",
    "region_sales_ranked_df.select('Region', 'Item Type', 'rn').show()\n",
    "\n",
    "# final result\n",
    "top_famous_items_df = region_sales_ranked_df\\\n",
    "    .withColumn('Item List', collect_list('Item Type').over(W.partitionBy('Region')))\\\n",
    "    .select('Region', 'Item List')\\\n",
    "    .distinct()\n",
    "\n",
    "top_famous_items_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d92e4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "top_famous_items_df.toPandas().to_csv('output/sales/top_famous_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea31b7",
   "metadata": {},
   "source": [
    "## 5. Find all the regions and their famous sales channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860db877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|              Region|   Sales Channels|\n",
      "+--------------------+-----------------+\n",
      "|                Asia|[Online, Offline]|\n",
      "|Australia and Oce...|[Online, Offline]|\n",
      "|Central America a...|[Online, Offline]|\n",
      "|              Europe|[Online, Offline]|\n",
      "|Middle East and N...|[Online, Offline]|\n",
      "|       North America|[Online, Offline]|\n",
      "|  Sub-Saharan Africa|[Online, Offline]|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions_sales_channels_df = sales_df\\\n",
    "    .withColumn('Sales Channels', collect_set('Sales Channel').over(W.partitionBy('Region')))\\\n",
    "    .select('Region', 'Sales Channels')\\\n",
    "    .distinct()\n",
    "\n",
    "regions_sales_channels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9733a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "regions_sales_channels_df.toPandas().to_csv('output/sales/region_sales_channels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98caa7",
   "metadata": {},
   "source": [
    "## 6. Find  the list of countries and items and their respective units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708e514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|    Country|    Item Type|Units Sold|\n",
      "+-----------+-------------+----------+\n",
      "|Afghanistan|    Household|    261953|\n",
      "|Afghanistan|Personal Care|    255956|\n",
      "|Afghanistan|       Cereal|    256936|\n",
      "|Afghanistan|    Baby Food|    232084|\n",
      "|Afghanistan|      Clothes|    220429|\n",
      "+-----------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_items_df = sales_df\\\n",
    "                        .groupBy('Country', 'Item Type')\\\n",
    "                        .agg(sum_('Units Sold')\\\n",
    "                        .alias('Units Sold'))\\\n",
    "                        .orderBy('Country')\n",
    "countries_items_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed1f46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "countries_items_df.toPandas().to_csv('output/sales/countries_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c238e",
   "metadata": {},
   "source": [
    "## 7. In 2013, identify the regions which sold maximum and minimum units of item type Meat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95d40e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|              Region|Item Type|Units Sold|\n",
      "+--------------------+---------+----------+\n",
      "|       North America|     Meat|    106193|\n",
      "|Australia and Oce...|     Meat|    449346|\n",
      "|Central America a...|     Meat|    615706|\n",
      "|Middle East and N...|     Meat|    745940|\n",
      "|                Asia|     Meat|    956367|\n",
      "|              Europe|     Meat|   1468932|\n",
      "|  Sub-Saharan Africa|     Meat|   1491277|\n",
      "+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, max as max_, min as min_\n",
    "\n",
    "sales_13_df = sales_df\\\n",
    "                .withColumn('year', substring('Order Date', -4, 4))\\\n",
    "                .where(col('year') == '2013')\n",
    "\n",
    "# sales_13_df.show()\n",
    "\n",
    "meat_sales_grouped_region_df = sales_13_df\\\n",
    "                                .where(col('Item Type') == 'Meat')\\\n",
    "                                .groupBy('Region', 'Item Type')\\\n",
    "                                .agg(sum_('Units Sold').alias('Units Sold'))\n",
    "\n",
    "meat_sales_grouped_region_df\\\n",
    "    .orderBy('Units Sold')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821213cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "meat_sales_grouped_region_df.orderBy('Units Sold').toPandas().to_csv('output/sales/meat_sales_units.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7b478",
   "metadata": {},
   "source": [
    "## 8. List all the items whose unit cost is less than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c684ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| Item Type|Unit Cost|\n",
      "+----------+---------+\n",
      "|    Cereal|   117.11|\n",
      "|      Meat|   364.69|\n",
      "| Baby Food|   159.42|\n",
      "|    Fruits|     6.92|\n",
      "|Vegetables|    90.93|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unit_cost_lt_500_df = sales_df.filter(col('Unit Cost') < 500).select('Item Type', 'Unit Cost').distinct()\n",
    "unit_cost_lt_500_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2449bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "unit_cost_lt_500_df.toPandas().to_csv('output/sales/unit_cost_lt_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2246e",
   "metadata": {},
   "source": [
    "## 9. Find the total cost, revenue and profit of each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d02a9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|Year|          Total Cost|       Total Revenue|        Total Profit|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|2010|1.233298120617372...|1.752972613683793...|  5.19674493173579E9|\n",
      "|2011|1.233635124975034...|1.751684120852689...| 5.180489954286942E9|\n",
      "|2012|1.245034203973387...|1.762118501057573...| 5.170842975163134E9|\n",
      "|2013|1.254475780787666...|1.780262842300091...| 5.257870612787804E9|\n",
      "|2014| 1.26472629863539E10|1.786939230240254...| 5.222129311173204E9|\n",
      "|2015|1.256502197052444...|1.779198425534374...| 5.226962291379013E9|\n",
      "|2016|1.229711736956142...|1.746406771565606...| 5.166950335527011E9|\n",
      "|2017| 7.018850991852474E9|1.000281918959213...|2.9839682052368097E9|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_sales_df = sales_df\\\n",
    "                    .withColumn('Year', substring('Order Date', -4, 4))\\\n",
    "\n",
    "totals_each_yr_df = year_sales_df\\\n",
    "    .groupBy('Year')\\\n",
    "    .agg(sum_('Total Cost').alias('Total Cost'), sum_('Total Revenue').alias('Total Revenue'), sum_('Total Profit').alias('Total Profit'))\\\n",
    "    .orderBy('Year')\n",
    "\n",
    "totals_each_yr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9c6d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "totals_each_yr_df.toPandas().to_csv('output/sales/total_each_yr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
