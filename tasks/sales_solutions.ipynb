{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76303d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total cost, total revenue, total profit on the basis of each region\n",
    "# Find the Item List on the basis of each country\n",
    "# Find the total number of items sold in each country\n",
    "# Find the top five famous item lists on the basis of each region.(Consider units sold while doing this.)\n",
    "# Find all the regions and their famous sales channels.\n",
    "# Find  the list of countries and items and their respective units.\n",
    "# In 2013, identify the regions which sold maximum and minimum units of item type Meat.\n",
    "# List all the items whose unit cost is less than 500\n",
    "# Find the total cost, revenue and profit of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1803b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2df1f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 18:19:53 WARN Utils: Your hostname, tars resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface wlan0)\n",
      "22/10/19 18:19:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 18:19:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f77b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 9) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item Type: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Sales Channel: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Total Cost: string (nullable = true)\n",
      " |-- Total Profit: string (nullable = true)\n",
      " |-- Total Revenue: string (nullable = true)\n",
      " |-- Unit Cost: string (nullable = true)\n",
      " |-- Unit Price: string (nullable = true)\n",
      " |-- Units Sold: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.format('json').load('data/sales_records.json')\n",
    "sales_df.printSchema()\n",
    "# sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bf66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item Type: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Sales Channel: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Total Cost: float (nullable = true)\n",
      " |-- Total Profit: float (nullable = true)\n",
      " |-- Total Revenue: float (nullable = true)\n",
      " |-- Unit Cost: float (nullable = true)\n",
      " |-- Unit Price: float (nullable = true)\n",
      " |-- Units Sold: integer (nullable = true)\n",
      " |-- Corrupt Record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all records are in the form of strings, so we need to do type casting\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "sales_df = sales_df\\\n",
    "            .withColumn('Total Cost', col('Total Cost').cast(FloatType()))\\\n",
    "            .withColumn('Total Profit', col('Total Profit').cast(FloatType()))\\\n",
    "            .withColumn('Total Revenue', col('Total Revenue').cast(FloatType()))\\\n",
    "            .withColumn('Unit Cost', col('Unit Cost').cast(FloatType()))\\\n",
    "            .withColumn('Unit Price', col('Unit Price').cast(FloatType()))\\\n",
    "            .withColumn('Units Sold', col('Units Sold').cast(IntegerType()))\\\n",
    "            .withColumnRenamed('_corrupt_record', 'Corrupt Record')\n",
    "\n",
    "sales_df = sales_df.na.drop(subset=['Region'])\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216ff32",
   "metadata": {},
   "source": [
    "## 1. Find the total cost, total revenue, total profit on the basis of each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7652b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 9) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              Region|          Total Cost|       Total Revenue|        Total Profit|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Middle East and N...|1.194003115362056...|1.691834583280536...| 4.978314680986322E9|\n",
      "|Australia and Oce...| 7.526098663258995E9|1.070152222371284...|3.1754235603909473E9|\n",
      "|              Europe|2.415937816242763E10|3.423977049206286...|1.008039233312204...|\n",
      "|  Sub-Saharan Africa|2.465031758112777...|3.495487197307492E10|1.030455438786905...|\n",
      "|Central America a...|1.026651963972375...|1.455373016365411...|4.2872105216859627E9|\n",
      "|       North America| 2.064450719133194E9|2.9370023334700775E9| 8.725516170647297E8|\n",
      "|                Asia|1.358588970253493...|1.929340122315577...|5.7075115161706505E9|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sum_, col\n",
    "\n",
    "sales_df\\\n",
    "    .groupBy('Region')\\\n",
    "    .agg(sum_(col('Total Cost')).alias('Total Cost'), sum_(col('Total Revenue')).alias('Total Revenue'), sum_('Total Profit').alias('Total Profit'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c02cb",
   "metadata": {},
   "source": [
    "## 2. Find the Item List on the basis of each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "557aadc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             Country|           Item List|\n",
      "+--------------------+--------------------+\n",
      "|         Afghanistan|[Beverages, Perso...|\n",
      "|             Albania|[Beverages, Perso...|\n",
      "|             Algeria|[Beverages, Perso...|\n",
      "|             Andorra|[Beverages, Perso...|\n",
      "|              Angola|[Beverages, Perso...|\n",
      "|Antigua and Barbuda |[Beverages, Perso...|\n",
      "|             Armenia|[Beverages, Perso...|\n",
      "|           Australia|[Beverages, Perso...|\n",
      "|             Austria|[Beverages, Perso...|\n",
      "|          Azerbaijan|[Beverages, Perso...|\n",
      "|             Bahrain|[Beverages, Perso...|\n",
      "|          Bangladesh|[Beverages, Perso...|\n",
      "|            Barbados|[Beverages, Perso...|\n",
      "|             Belarus|[Beverages, Perso...|\n",
      "|             Belgium|[Beverages, Perso...|\n",
      "|              Belize|[Beverages, Perso...|\n",
      "|               Benin|[Beverages, Perso...|\n",
      "|              Bhutan|[Beverages, Perso...|\n",
      "|Bosnia and Herzeg...|[Beverages, Perso...|\n",
      "|            Botswana|[Beverages, Perso...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assuming item type list\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.functions import collect_list, collect_set, explode\n",
    "\n",
    "window_spec = W.partitionBy('Country')\n",
    "country_item_list_df = sales_df\\\n",
    "                        .withColumn('Item List', collect_set('Item Type').over(window_spec))\\\n",
    "                        .select('Country', 'Item List')\\\n",
    "                        .distinct()\n",
    "\n",
    "country_item_list_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebfa77",
   "metadata": {},
   "source": [
    "## 3. Find the total number of items sold in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b06c59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|    Country|Num items sold|\n",
      "+-----------+--------------+\n",
      "|       Chad|       2660461|\n",
      "|     Russia|       2579558|\n",
      "|      Yemen|       2966519|\n",
      "|    Senegal|       2716010|\n",
      "|     Sweden|       2698756|\n",
      "|   Kiribati|       2555774|\n",
      "|    Eritrea|       2552497|\n",
      "|Philippines|       2610149|\n",
      "|   Djibouti|       2699545|\n",
      "|      Tonga|       2565238|\n",
      "|  Singapore|       2693579|\n",
      "|   Malaysia|       2587267|\n",
      "|       Fiji|       2613373|\n",
      "|     Turkey|       2732629|\n",
      "|     Malawi|       2645975|\n",
      "|       Iraq|       2765491|\n",
      "|    Germany|       2502470|\n",
      "|    Comoros|       2556790|\n",
      "|   Cambodia|       2946963|\n",
      "|Afghanistan|       2805640|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we need to use units sold for this\n",
    "from pyspark.sql.functions import sum as sum_\n",
    "sales_df\\\n",
    "    .groupBy('Country')\\\n",
    "    .agg(sum_('Units Sold').alias('Num items sold'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983067c7",
   "metadata": {},
   "source": [
    "## 4. Find the top five famous items list on the basis of each region.(Consider units sold while doing this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b1ceae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|              Region|      Item Type|\n",
      "+--------------------+---------------+\n",
      "|                Asia|         Cereal|\n",
      "|                Asia|         Snacks|\n",
      "|                Asia|Office Supplies|\n",
      "|                Asia|     Vegetables|\n",
      "|                Asia|        Clothes|\n",
      "|Australia and Oce...|  Personal Care|\n",
      "|Australia and Oce...|     Vegetables|\n",
      "|Australia and Oce...|         Cereal|\n",
      "|Australia and Oce...|      Beverages|\n",
      "|Australia and Oce...|        Clothes|\n",
      "|Central America a...|      Cosmetics|\n",
      "|Central America a...|        Clothes|\n",
      "|Central America a...|           Meat|\n",
      "|Central America a...|Office Supplies|\n",
      "|Central America a...|      Baby Food|\n",
      "|              Europe|         Cereal|\n",
      "|              Europe|Office Supplies|\n",
      "|              Europe|     Vegetables|\n",
      "|              Europe|      Beverages|\n",
      "|              Europe|         Fruits|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              Region|           Item List|\n",
      "+--------------------+--------------------+\n",
      "|                Asia|[Cereal, Snacks, ...|\n",
      "|Australia and Oce...|[Personal Care, V...|\n",
      "|Central America a...|[Cosmetics, Cloth...|\n",
      "|              Europe|[Cereal, Office S...|\n",
      "|Middle East and N...|[Office Supplies,...|\n",
      "|       North America|[Cosmetics, House...|\n",
      "|  Sub-Saharan Africa|[Cosmetics, Baby ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, rank\n",
    "\n",
    "region_sales_df = sales_df\\\n",
    "                    .groupBy('Region', 'Item Type')\\\n",
    "                    .agg(sum_('Units Sold').alias('Total Units Sold'))\\\n",
    "                    .orderBy('Region')\n",
    "\n",
    "window_spec = W.partitionBy('Region')\\\n",
    "                .orderBy(col('Total Units Sold').desc())\n",
    "\n",
    "region_sales_ranked_df = region_sales_df\\\n",
    "                            .withColumn('rn', rank().over(window_spec))\\\n",
    "                            .where('rn <= 5')\\\n",
    "                            .drop('rn')\n",
    "\n",
    "# regions and their five best item types\n",
    "region_sales_ranked_df.select('Region', 'Item Type').show()\n",
    "\n",
    "# final result\n",
    "region_sales_ranked_df\\\n",
    "    .withColumn('Item List', collect_list('Item Type').over(W.partitionBy('Region')))\\\n",
    "    .select('Region', 'Item List')\\\n",
    "    .distinct()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea31b7",
   "metadata": {},
   "source": [
    "## 5. Find all the regions and their famous sales channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "860db877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|              Region|   Sales Channels|\n",
      "+--------------------+-----------------+\n",
      "|                Asia|[Online, Offline]|\n",
      "|Australia and Oce...|[Online, Offline]|\n",
      "|Central America a...|[Online, Offline]|\n",
      "|              Europe|[Online, Offline]|\n",
      "|Middle East and N...|[Online, Offline]|\n",
      "|       North America|[Online, Offline]|\n",
      "|  Sub-Saharan Africa|[Online, Offline]|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df\\\n",
    "    .withColumn('Sales Channels', collect_set('Sales Channel').over(W.partitionBy('Region')))\\\n",
    "    .select('Region', 'Sales Channels')\\\n",
    "    .distinct()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98caa7",
   "metadata": {},
   "source": [
    "## 6. Find  the list of countries and items and their respective units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "708e514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+\n",
      "|    Country|      Item Type|Units Sold|\n",
      "+-----------+---------------+----------+\n",
      "|Afghanistan|     Vegetables|    219937|\n",
      "|Afghanistan|      Cosmetics|    217192|\n",
      "|Afghanistan|Office Supplies|    166911|\n",
      "|Afghanistan|           Meat|    273402|\n",
      "|Afghanistan|        Clothes|    220429|\n",
      "|Afghanistan|  Personal Care|    255956|\n",
      "|Afghanistan|         Cereal|    256936|\n",
      "|Afghanistan|      Beverages|    206154|\n",
      "|Afghanistan|      Household|    261953|\n",
      "|Afghanistan|         Fruits|    257336|\n",
      "|Afghanistan|         Snacks|    237350|\n",
      "|Afghanistan|      Baby Food|    232084|\n",
      "|    Albania|         Cereal|    215238|\n",
      "|    Albania|           Meat|    266123|\n",
      "|    Albania|        Clothes|    250884|\n",
      "|    Albania|         Snacks|    210384|\n",
      "|    Albania|Office Supplies|    236822|\n",
      "|    Albania|      Baby Food|    191480|\n",
      "|    Albania|         Fruits|    252283|\n",
      "|    Albania|  Personal Care|    222874|\n",
      "+-----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy('Country', 'Item Type').agg(sum_('Units Sold').alias('Units Sold')).orderBy('Country').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c238e",
   "metadata": {},
   "source": [
    "## 7. In 2013, identify the regions which sold maximum and minimum units of item type Meat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95d40e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|              Region|Item Type|Units Sold|\n",
      "+--------------------+---------+----------+\n",
      "|       North America|     Meat|    106193|\n",
      "|Australia and Oce...|     Meat|    449346|\n",
      "|Central America a...|     Meat|    615706|\n",
      "|Middle East and N...|     Meat|    745940|\n",
      "|                Asia|     Meat|    956367|\n",
      "|              Europe|     Meat|   1468932|\n",
      "|  Sub-Saharan Africa|     Meat|   1491277|\n",
      "+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, max as max_, min as min_\n",
    "\n",
    "sales_13_df = sales_df\\\n",
    "                .withColumn('year', substring('Order Date', -4, 4))\\\n",
    "                .where(col('year') == '2013')\n",
    "\n",
    "# sales_13_df.show()\n",
    "\n",
    "meat_sales_grouped_region_df = sales_13_df\\\n",
    "                                .where(col('Item Type') == 'Meat')\\\n",
    "                                .groupBy('Region', 'Item Type')\\\n",
    "                                .agg(sum_('Units Sold').alias('Units Sold'))\n",
    "\n",
    "meat_sales_grouped_region_df\\\n",
    "    .orderBy('Units Sold')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7b478",
   "metadata": {},
   "source": [
    "## 8. List all the items whose unit cost is less than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c684ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|    Item Type|Unit Cost|\n",
      "+-------------+---------+\n",
      "|       Cereal|   117.11|\n",
      "|         Meat|   364.69|\n",
      "|    Baby Food|   159.42|\n",
      "|       Fruits|     6.92|\n",
      "|   Vegetables|    90.93|\n",
      "|    Beverages|    31.79|\n",
      "|Personal Care|    56.67|\n",
      "|      Clothes|    35.84|\n",
      "|    Cosmetics|   263.33|\n",
      "|       Snacks|    97.44|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.filter(col('Unit Cost') < 500).select('Item Type', 'Unit Cost').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2246e",
   "metadata": {},
   "source": [
    "## 9. Find the total cost, revenue and profit of each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d02a9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|Year|          Total Cost|       Total Revenue|        Total Profit|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|2010|1.233298120617372...|1.752972613683793...|  5.19674493173579E9|\n",
      "|2011|1.233635124975034...|1.751684120852689...| 5.180489954286942E9|\n",
      "|2012|1.245034203973387...|1.762118501057573...| 5.170842975163134E9|\n",
      "|2013|1.254475780787666...|1.780262842300091...| 5.257870612787804E9|\n",
      "|2014| 1.26472629863539E10|1.786939230240254...| 5.222129311173204E9|\n",
      "|2015|1.256502197052444...|1.779198425534374...| 5.226962291379013E9|\n",
      "|2016|1.229711736956142...|1.746406771565606...| 5.166950335527011E9|\n",
      "|2017| 7.018850991852474E9|1.000281918959213...|2.9839682052368097E9|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_sales_df = sales_df\\\n",
    "                    .withColumn('Year', substring('Order Date', -4, 4))\\\n",
    "\n",
    "year_sales_df\\\n",
    "    .groupBy('Year')\\\n",
    "    .agg(sum_('Total Cost').alias('Total Cost'), sum_('Total Revenue').alias('Total Revenue'), sum_('Total Profit').alias('Total Profit'))\\\n",
    "    .orderBy('Year')\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
